{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06584840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139da3d1",
   "metadata": {},
   "source": [
    "# Description of the target variable (number of pickup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85350473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Locker Name</th>\n",
       "      <th>Date Hour</th>\n",
       "      <th>IsIndoor</th>\n",
       "      <th>size_L_delivery</th>\n",
       "      <th>size_M_delivery</th>\n",
       "      <th>size_S_delivery</th>\n",
       "      <th>size_XL_delivery</th>\n",
       "      <th>size_L_withdraw</th>\n",
       "      <th>size_M_withdraw</th>\n",
       "      <th>size_S_withdraw</th>\n",
       "      <th>size_XL_withdraw</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>DBSCAN Cluster</th>\n",
       "      <th>KMeans Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Locker</td>\n",
       "      <td>2024-04-10 18:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Total Locker</td>\n",
       "      <td>2024-04-10 19:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total Locker</td>\n",
       "      <td>2024-04-11 12:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Total Locker</td>\n",
       "      <td>2024-04-11 14:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total Locker</td>\n",
       "      <td>2024-04-11 15:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Locker Name           Date Hour  IsIndoor  size_L_delivery  \\\n",
       "0  Total Locker 2024-04-10 18:00:00         1              0.0   \n",
       "1  Total Locker 2024-04-10 19:00:00         1              0.0   \n",
       "2  Total Locker 2024-04-11 12:00:00         1              0.0   \n",
       "3  Total Locker 2024-04-11 14:00:00         1              0.0   \n",
       "4  Total Locker 2024-04-11 15:00:00         1              0.0   \n",
       "\n",
       "   size_M_delivery  size_S_delivery  size_XL_delivery  size_L_withdraw  \\\n",
       "0              1.0              6.0               0.0              0.0   \n",
       "1              0.0              0.0               0.0              0.0   \n",
       "2              0.0              0.0               0.0              0.0   \n",
       "3              0.0              0.0               0.0              0.0   \n",
       "4              0.0              0.0               0.0              0.0   \n",
       "\n",
       "   size_M_withdraw  size_S_withdraw  size_XL_withdraw  IsHoliday  \\\n",
       "0              0.0              0.0               0.0          0   \n",
       "1              0.0              1.0               0.0          0   \n",
       "2              1.0              1.0               0.0          0   \n",
       "3              0.0              1.0               0.0          0   \n",
       "4              0.0              2.0               0.0          0   \n",
       "\n",
       "   DBSCAN Cluster  KMeans Cluster  \n",
       "0               0               4  \n",
       "1               0               4  \n",
       "2               0               4  \n",
       "3               0               4  \n",
       "4               0               4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/locker_nyc_engineered.csv')\n",
    "# Convert Date Hour to datetime\n",
    "df['Date Hour'] = pd.to_datetime(df['Date Hour'])\n",
    "df['Locker Name'] = \"Total Locker\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a5f2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([np.int64(4), np.int64(3), np.int64(1), np.int64(0), np.int64(2)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cluster Dataframe: Split the records by \"KMeans Cluster\" feature\n",
    "clustered_dfs = {cluster: df[df['KMeans Cluster'] == cluster] for cluster in df['KMeans Cluster'].unique()}\n",
    "clustered_dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786a3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resample(df):\n",
    "    # Group by Locker Name, resample to 3 hours, and take the sum of numerical columns\n",
    "    df = df.set_index('Date Hour')\n",
    "    df = df.groupby('Locker Name').resample('3h').sum().drop(columns=[\"Locker Name\"])\n",
    "    df = df.reset_index()\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "def _rolling_window_sum(df):\n",
    "    # Apply rolling window\n",
    "    for window in [1, 2]:\n",
    "        df[f'withdraw_{window}'] = df.groupby('Locker Name')['size_S_withdraw'].transform(lambda x: x.shift(1).rolling(window=window, min_periods=1).sum())\n",
    "        # Fill NaN values with 0\n",
    "        df[f'withdraw_{window}'] = df[f'withdraw_{window}'].fillna(0)\n",
    "\n",
    "    # Apply rolling window\n",
    "    for window in [1, 2, 8, 16]:\n",
    "        df[f'delivery_{window}'] = df.groupby('Locker Name')['size_S_delivery'].transform(lambda x: x.rolling(window=window, min_periods=1).sum())\n",
    "\n",
    "    return df\n",
    "\n",
    "def _transform_target(df):\n",
    "    # Proxy inventory by applying rolling window sum of withdraw - delivery\n",
    "    df[f'inventory'] = df[f'delivery_1'] - df[f'withdraw_1']\n",
    "    # Apply cumulative sum to get a proxy inventory level\n",
    "    df[f'inventory'] = df.groupby('Locker Name')[f'inventory'].cumsum()\n",
    "\n",
    "    # Target variable: proportion_withdraw: size_S_withdraw / inventory\n",
    "    df['proportion_withdraw'] = df.apply(lambda row: row['size_S_withdraw'] / row['inventory'] if row['inventory'] > 0 else 0, axis=1)\n",
    "    # df.drop(columns=['size_S_withdraw'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def _cyclical_features(df):\n",
    "    # Create cyclical features for Hour, Day of Week, Month\n",
    "    # MONTHS_IN_YEAR = 12\n",
    "    HOURS_IN_DAY = 24\n",
    "    # DAYS_IN_WEEK = 7\n",
    "    # QUARTERS_IN_YEAR = 4\n",
    "\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['Hour'] / HOURS_IN_DAY)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['Hour'] / HOURS_IN_DAY)\n",
    "    # df['month_sin'] = np.sin(2 * np.pi * df['Month'] / MONTHS_IN_YEAR)\n",
    "    # df['month_cos'] = np.cos(2 * np.pi * df['Month'] / MONTHS_IN_YEAR)\n",
    "    # df['day_of_week_sin'] = np.sin(2 * np.pi * df['Day of Week'] / DAYS_IN_WEEK)\n",
    "    # df['day_of_week_cos'] = np.cos(2 * np.pi * df['Day of Week'] / DAYS_IN_WEEK)\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    # Extract dict Locker Name -> IsIndoor\n",
    "    locker_indoor_dict = df.set_index('Locker Name')['IsIndoor'].to_dict()\n",
    "    # Cache IsHoliday per date\n",
    "    holiday_dict = df.groupby(df['Date Hour'].dt.date)['IsHoliday'].first().to_dict()\n",
    "    df = df.drop(columns=['IsIndoor'])\n",
    "\n",
    "    df = _resample(df)\n",
    "\n",
    "    # Map IsHoliday back to the resampled dataframe\n",
    "    df['IsHoliday'] = df['Date Hour'].dt.date.map(holiday_dict)\n",
    "\n",
    "    df = _rolling_window_sum(df)\n",
    "    df = _transform_target(df)\n",
    "\n",
    "    # Ignore all size_*_delivery and size_*_withdraw columns except size_S_withdraw\n",
    "    cols_to_drop = [col for col in df.columns if ('size_' in col and col not in ['size_S_withdraw'])]\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Apply the IsIndoor feature back to the resampled dataframe\n",
    "    df['IsIndoor'] = df['Locker Name'].map(locker_indoor_dict)\n",
    "    df['Hour'] = df['Date Hour'].dt.hour\n",
    "    # df['Day of Week'] = df['Date Hour'].dt.dayofweek\n",
    "    df['Month'] = df['Date Hour'].dt.month\n",
    "    df['IsPeakHour'] = df['Date Hour'].apply(lambda x: True if 17 <= x.hour <= 20 else False)\n",
    "    df['IsWeekend'] = df['Date Hour'].dt.dayofweek.apply(lambda x: True if x >= 5 else False)\n",
    "\n",
    "    # Create a time index as incremental integer\n",
    "    df['time_idx'] = df.groupby('Locker Name')['Date Hour'].transform(lambda x: (x - x.min()).dt.total_seconds() // 3600 / 3).astype(int)\n",
    "\n",
    "    df = _cyclical_features(df)\n",
    "\n",
    "    df = df.drop(columns=['Hour', 'Month', 'IsHoliday'])\n",
    "\n",
    "    # Remove the cluster columns\n",
    "    df = df.drop(columns=['DBSCAN Cluster', 'KMeans Cluster'])\n",
    "    return df\n",
    "\n",
    "# Apply preprocessing to each clustered dataframe and concatenate them back\n",
    "clustered_dfs = {cluster: preprocess(clustered_dfs[cluster]) for cluster in clustered_dfs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a95240f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchmetrics import MeanSquaredError\n",
    "import math\n",
    "\n",
    "# Custom Dataset for time series data from DataFrame\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, encoder_length, decoder_length, target_col, feature_cols, from_train_scaler=None):\n",
    "        self.encoder_length = encoder_length\n",
    "        self.decoder_length = decoder_length\n",
    "        self.target_col = target_col\n",
    "        self.feature_cols = feature_cols\n",
    "        \n",
    "        # Scale features\n",
    "        if from_train_scaler:\n",
    "            self.scaler = from_train_scaler\n",
    "            self.features = self.scaler.transform(df[feature_cols].values)\n",
    "        else:\n",
    "             self.scaler = StandardScaler()\n",
    "             self.features = self.scaler.fit_transform(df[feature_cols].values)\n",
    "        self.indices = df['time_idx'].values\n",
    "        self.dates = df['Date Hour'].values\n",
    "        self.targets = df[target_col].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.encoder_length - self.decoder_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence of features and target\n",
    "        x = self.features[idx:idx+self.encoder_length]\n",
    "        y = self.targets[idx+self.encoder_length:idx+self.encoder_length+self.decoder_length]\n",
    "        index = self.indices[idx+self.encoder_length:idx+self.encoder_length+self.decoder_length]\n",
    "        \n",
    "        return (torch.tensor(x, dtype=torch.float32), \n",
    "                torch.tensor(y, dtype=torch.float32),\n",
    "                torch.tensor(index, dtype=torch.int64)\n",
    "        )\n",
    "\n",
    "class DecayMSE(nn.Module):\n",
    "    def __init__(self, lambda_decay=0.1):\n",
    "        super(DecayMSE, self).__init__()\n",
    "        self.lambda_decay = lambda_decay\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate the squared errors\n",
    "        squared_errors = self.mse(y_pred, y_true)\n",
    "        H = squared_errors.size(1)\n",
    "        # Create decay weights: w_h = exp(-λ * h) for h = 1 to H\n",
    "        decay_factors = torch.tensor([math.exp(-self.lambda_decay * i) for i in range(H)], device=squared_errors.device)\n",
    "        # Apply the decay factors to the squared errors\n",
    "        weighted_errors = squared_errors * decay_factors\n",
    "        # Return the mean of the weighted errors\n",
    "        return weighted_errors.mean()\n",
    "\n",
    "# Plain PyTorch LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, output_horizon, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_horizon)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Function to create DataLoader from DataFrame\n",
    "def create_dataloader(df, encoder_length, decoder_length, target_col, feature_cols, batch_size=32, train_val_test_split=(0.7, 0.2, 0.1), num_workers=0):\n",
    "    train_split, val_split, test_split = train_val_test_split\n",
    "\n",
    "    # Split data into train and validation\n",
    "    train_size = int(train_split * len(df))\n",
    "    train_df = df[:train_size]\n",
    "    val_df = df[train_size:train_size + int(val_split * len(df))]\n",
    "    test_df = df[train_size + int(val_split * len(df)):]\n",
    "    print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "    # Create datasets\n",
    "    train_dataset = TimeSeriesDataset(train_df, encoder_length, decoder_length, target_col, feature_cols)\n",
    "    val_dataset = TimeSeriesDataset(val_df, encoder_length, decoder_length, target_col, feature_cols, from_train_scaler=train_dataset.scaler)\n",
    "    test_dataset = TimeSeriesDataset(test_df, encoder_length, decoder_length, target_col, feature_cols, from_train_scaler=train_dataset.scaler)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd53a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dc875d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2516, Validation size: 719, Test size: 360\n"
     ]
    }
   ],
   "source": [
    "ENCODER_LENGTH = 16\n",
    "DECODER_LENGTH = 4\n",
    "TARGET_COL = 'proportion_withdraw'\n",
    "FEATURE_COLS = [col for col in df.columns if col not in ['Date Hour', 'Locker Name', 'proportion_withdraw', 'time_idx', 'size_S_withdraw']]\n",
    "BATCH_SIZE = 128\n",
    "INPUT_SIZE = len(FEATURE_COLS)\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 8\n",
    "WORKERS = 0\n",
    "\n",
    "train_loader, val_loader, test_loader, train_df, val_df, test_df = create_dataloader(\n",
    "    df,\n",
    "    encoder_length=ENCODER_LENGTH,\n",
    "    decoder_length=DECODER_LENGTH,\n",
    "    target_col=TARGET_COL,\n",
    "    feature_cols=FEATURE_COLS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_val_test_split=(0.7, 0.2, 0.1),\n",
    "    num_workers=WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6582e041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Locker Name</th>\n",
       "      <th>Date Hour</th>\n",
       "      <th>size_S_withdraw</th>\n",
       "      <th>withdraw_1</th>\n",
       "      <th>withdraw_2</th>\n",
       "      <th>delivery_1</th>\n",
       "      <th>delivery_2</th>\n",
       "      <th>delivery_8</th>\n",
       "      <th>delivery_16</th>\n",
       "      <th>inventory</th>\n",
       "      <th>proportion_withdraw</th>\n",
       "      <th>IsIndoor</th>\n",
       "      <th>IsPeakHour</th>\n",
       "      <th>IsWeekend</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Locker</td>\n",
       "      <td>2024-04-10 12:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Total Locker</td>\n",
       "      <td>2024-04-10 15:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total Locker</td>\n",
       "      <td>2024-04-10 18:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Total Locker</td>\n",
       "      <td>2024-04-10 21:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>7.071068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total Locker</td>\n",
       "      <td>2024-04-11 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Locker Name           Date Hour  size_S_withdraw  withdraw_1  withdraw_2  \\\n",
       "0  Total Locker 2024-04-10 12:00:00              2.0         0.0         0.0   \n",
       "1  Total Locker 2024-04-10 15:00:00              0.0         2.0         2.0   \n",
       "2  Total Locker 2024-04-10 18:00:00              1.0         0.0         2.0   \n",
       "3  Total Locker 2024-04-10 21:00:00              0.0         1.0         1.0   \n",
       "4  Total Locker 2024-04-11 00:00:00              0.0         0.0         1.0   \n",
       "\n",
       "   delivery_1  delivery_2  delivery_8  delivery_16  inventory  \\\n",
       "0         2.0         2.0         2.0          2.0        2.0   \n",
       "1         0.0         2.0         2.0          2.0        0.0   \n",
       "2        10.0        10.0        12.0         12.0       10.0   \n",
       "3         0.0        10.0        12.0         12.0        9.0   \n",
       "4         0.0         0.0        12.0         12.0        9.0   \n",
       "\n",
       "   proportion_withdraw  IsIndoor  IsPeakHour  IsWeekend  time_idx  \\\n",
       "0                  1.0         0       False      False         0   \n",
       "1                  0.0         0       False      False         1   \n",
       "2                  0.1         0        True      False         2   \n",
       "3                  0.0         0       False      False         3   \n",
       "4                  0.0         0       False      False         4   \n",
       "\n",
       "       hour_sin      hour_cos  \n",
       "0  1.224647e-16 -1.000000e+00  \n",
       "1 -7.071068e-01 -7.071068e-01  \n",
       "2 -1.000000e+00 -1.836970e-16  \n",
       "3 -7.071068e-01  7.071068e-01  \n",
       "4  0.000000e+00  1.000000e+00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e6d4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 3961860 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(\n",
    "    input_size=INPUT_SIZE,\n",
    "    output_horizon=DECODER_LENGTH,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    ")\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecc26bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([128, 16, 12]), Target shape: torch.Size([128, 4])\n"
     ]
    }
   ],
   "source": [
    "# Load the first batch to check for correct matching of input and output shapes\n",
    "for batch in train_loader:\n",
    "    x, y, idx = batch\n",
    "    print(f\"Input shape: {x.shape}, Target shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2051feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# trainer = pl.Trainer(\n",
    "#     accelerator=\"gpu\",\n",
    "#     devices=1,\n",
    "#     min_epochs=10,\n",
    "#     max_epochs=500,\n",
    "#     callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb3cbe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the number of batches in each loader\n",
    "# print(f\"Number of training batches: {len(train_loader)}\")\n",
    "# print(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "290c1b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c575740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18c2a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enhanced Residual Analysis: Regular and Studentized Residuals\n",
    "\n",
    "# import scipy.stats as stats\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Calculate residuals\n",
    "# residuals = evaluation_df['true_values'] - evaluation_df['predictions']\n",
    "\n",
    "# # Calculate studentized residuals\n",
    "# # Studentized residuals = residuals / sqrt(MSE * (1 - h_ii))\n",
    "# # where h_ii is the leverage (diagonal of hat matrix)\n",
    "\n",
    "# # For simplicity, we'll use a rolling standard deviation approach\n",
    "# # This approximates studentized residuals for time series data\n",
    "# residual_std = residuals.rolling(window=20, center=True, min_periods=5).std()\n",
    "# studentized_residuals = residuals / (residual_std + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "\n",
    "# # Alternative: Use overall standard deviation for studentized residuals\n",
    "# overall_std = residuals.std()\n",
    "# studentized_residuals_simple = residuals / overall_std\n",
    "\n",
    "# # Create comprehensive residual analysis plots\n",
    "# fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n",
    "# fig.suptitle('Comprehensive Residual Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# # 1. Regular Residuals Time Series\n",
    "# axes[0, 0].plot(evaluation_df['time_idx'], residuals, 'b-', alpha=0.7, linewidth=1)\n",
    "# axes[0, 0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "# axes[0, 0].set_title('Regular Residuals Over Time', fontweight='bold')\n",
    "# axes[0, 0].set_xlabel('Time Index')\n",
    "# axes[0, 0].set_ylabel('Residuals')\n",
    "# axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # 2. Regular Residuals Scatter Plot\n",
    "# axes[0, 1].scatter(evaluation_df['time_idx'], residuals, alpha=0.6, s=20, color='blue')\n",
    "# axes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "# axes[0, 1].axhline(y=2*np.std(residuals), color='orange', linestyle='--', alpha=0.6, label=f'+2σ ({2*np.std(residuals):.2f})')\n",
    "# axes[0, 1].axhline(y=-2*np.std(residuals), color='orange', linestyle='--', alpha=0.6, label=f'-2σ ({-2*np.std(residuals):.2f})')\n",
    "# axes[0, 1].set_title('Regular Residuals Scatter Plot', fontweight='bold')\n",
    "# axes[0, 1].set_xlabel('Time Index')\n",
    "# axes[0, 1].set_ylabel('Residuals')\n",
    "# axes[0, 1].legend()\n",
    "# axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# # 3. Studentized Residuals (Rolling Std) Time Series\n",
    "# axes[1, 0].plot(evaluation_df['time_idx'], studentized_residuals, 'g-', alpha=0.7, linewidth=1)\n",
    "# axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "# axes[1, 0].axhline(y=2, color='orange', linestyle='--', alpha=0.6, label='+2')\n",
    "# axes[1, 0].axhline(y=-2, color='orange', linestyle='--', alpha=0.6, label='-2')\n",
    "# axes[1, 0].axhline(y=3, color='red', linestyle='--', alpha=0.4, label='+3')\n",
    "# axes[1, 0].axhline(y=-3, color='red', linestyle='--', alpha=0.4, label='-3')\n",
    "# axes[1, 0].set_title('Studentized Residuals (Rolling Std) Over Time', fontweight='bold')\n",
    "# axes[1, 0].set_xlabel('Time Index')\n",
    "# axes[1, 0].set_ylabel('Studentized Residuals')\n",
    "# axes[1, 0].legend()\n",
    "# axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # 4. Studentized Residuals (Rolling Std) Scatter Plot\n",
    "# axes[1, 1].scatter(evaluation_df['time_idx'], studentized_residuals, alpha=0.6, s=20, color='green')\n",
    "# axes[1, 1].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "# axes[1, 1].axhline(y=2, color='orange', linestyle='--', alpha=0.6, label='±2 (Outlier threshold)')\n",
    "# axes[1, 1].axhline(y=-2, color='orange', linestyle='--', alpha=0.6)\n",
    "# axes[1, 1].axhline(y=3, color='red', linestyle='--', alpha=0.4, label='±3 (Extreme outlier)')\n",
    "# axes[1, 1].axhline(y=-3, color='red', linestyle='--', alpha=0.4)\n",
    "# axes[1, 1].set_title('Studentized Residuals (Rolling Std) Scatter Plot', fontweight='bold')\n",
    "# axes[1, 1].set_xlabel('Time Index')\n",
    "# axes[1, 1].set_ylabel('Studentized Residuals')\n",
    "# axes[1, 1].legend()\n",
    "# axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# # 5. Residuals Distribution Histogram\n",
    "# axes[2, 0].hist(residuals, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "# axes[2, 0].axvline(x=0, color='red', linestyle='--', alpha=0.8)\n",
    "# axes[2, 0].axvline(x=np.mean(residuals), color='green', linestyle='-', alpha=0.8, label=f'Mean: {np.mean(residuals):.3f}')\n",
    "# axes[2, 0].set_title('Residuals Distribution', fontweight='bold')\n",
    "# axes[2, 0].set_xlabel('Residual Value')\n",
    "# axes[2, 0].set_ylabel('Frequency')\n",
    "# axes[2, 0].legend()\n",
    "# axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # 6. Q-Q Plot for Normality Check\n",
    "# stats.probplot(residuals, dist=\"norm\", plot=axes[2, 1])\n",
    "# axes[2, 1].set_title('Q-Q Plot (Normality Check)', fontweight='bold')\n",
    "# axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Statistical Summary\n",
    "# print(\"=\" * 60)\n",
    "# print(\"RESIDUAL ANALYSIS SUMMARY\")\n",
    "# print(\"=\" * 60)\n",
    "# print(f\"Number of observations: {len(residuals)}\")\n",
    "# print(f\"Mean of residuals: {np.mean(residuals):.4f}\")\n",
    "# print(f\"Standard deviation of residuals: {np.std(residuals):.4f}\")\n",
    "# print(f\"Min residual: {np.min(residuals):.4f}\")\n",
    "# print(f\"Max residual: {np.max(residuals):.4f}\")\n",
    "# print(f\"Skewness: {stats.skew(residuals):.4f}\")\n",
    "# print(f\"Kurtosis: {stats.kurtosis(residuals):.4f}\")\n",
    "\n",
    "# # Outlier detection using studentized residuals\n",
    "# outliers_2sigma = np.abs(studentized_residuals) > 2\n",
    "# outliers_3sigma = np.abs(studentized_residuals) > 3\n",
    "\n",
    "# print(f\"\\nOutlier Analysis (Studentized Residuals):\")\n",
    "# print(f\"Points beyond ±2σ: {np.sum(outliers_2sigma)} ({100*np.mean(outliers_2sigma):.1f}%)\")\n",
    "# print(f\"Points beyond ±3σ: {np.sum(outliers_3sigma)} ({100*np.mean(outliers_3sigma):.1f}%)\")\n",
    "\n",
    "# # Identify potential outlier time indices\n",
    "# if np.sum(outliers_3sigma) > 0:\n",
    "#     outlier_indices = evaluation_df['time_idx'][outliers_3sigma].values\n",
    "#     print(f\"\\nExtreme outlier time indices: {outlier_indices[:10]}{'...' if len(outlier_indices) > 10 else ''}\")\n",
    "\n",
    "# # Autocorrelation of residuals (for time series)\n",
    "# from statsmodels.graphics.tsaplots import plot_acf\n",
    "# fig, ax = plt.subplots(figsize=(12, 6))\n",
    "# plot_acf(residuals, lags=50, ax=ax, alpha=0.05)\n",
    "# ax.set_title('Autocorrelation of Residuals', fontweight='bold')\n",
    "# ax.grid(True, alpha=0.3)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"\\nAutocorrelation analysis helps identify if residuals have temporal patterns.\")\n",
    "# print(\"Significant autocorrelation indicates the model may not capture all temporal dependencies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad08381",
   "metadata": {},
   "source": [
    "# Federated Learning Simulation Setup\n",
    "We'll simulate cross-silo FL with each KMeans cluster as a separate client. We'll use Flower for orchestration and a simple FedAvg strategy. If the LightningModule is incompatible, we'll reuse the same model architecture in plain PyTorch for the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7005095e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Install and import Flower (if not already installed)\n",
    "from flwr.client import NumPyClient, Client, ClientApp\n",
    "from flwr.common import Metrics, Context\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.simulation import run_simulation\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# DEVICE = \"cpu\" # Force CPU for testing\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Reuse model and dataset utilities from earlier cells\n",
    "# LSTMModel, DecayMSE, TimeSeriesDataset, create_dataloader are assumed to be defined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b4a8c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 5 clients from clusters: [np.int64(4), np.int64(3), np.int64(1), np.int64(0), np.int64(2)]\n"
     ]
    }
   ],
   "source": [
    "# Build per-client dataloaders from clustered_dfs\n",
    "ENCODER_LENGTH = 16\n",
    "DECODER_LENGTH = 4\n",
    "TARGET_COL = 'proportion_withdraw'\n",
    "FEATURE_COLS = [col for col in df.columns if col not in ['Date Hour', 'Locker Name', 'proportion_withdraw', 'time_idx', 'size_S_withdraw']]\n",
    "BATCH_SIZE = 128\n",
    "WORKERS = 0  # set to 0 for portability in notebooks\n",
    "\n",
    "client_splits = {}\n",
    "for cluster, df_client in clustered_dfs.items():\n",
    "    # Temporal split: 70/20/10 per client\n",
    "    n = len(df_client)\n",
    "    train_size = int(0.7 * n)\n",
    "    val_size = int(0.2 * n)\n",
    "    train_df = df_client.iloc[:train_size]\n",
    "    val_df = df_client.iloc[train_size: train_size + val_size]\n",
    "    test_df = df_client.iloc[train_size + val_size:]\n",
    "\n",
    "    # Use one scaler per client, fitted on train only; share to val/test\n",
    "    train_dataset = TimeSeriesDataset(train_df, ENCODER_LENGTH, DECODER_LENGTH, TARGET_COL, FEATURE_COLS)\n",
    "    val_dataset = TimeSeriesDataset(val_df, ENCODER_LENGTH, DECODER_LENGTH, TARGET_COL, FEATURE_COLS, from_train_scaler=train_dataset.scaler)\n",
    "    test_dataset = TimeSeriesDataset(test_df, ENCODER_LENGTH, DECODER_LENGTH, TARGET_COL, FEATURE_COLS, from_train_scaler=train_dataset.scaler)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS)\n",
    "\n",
    "    client_splits[cluster] = {\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'test_loader': test_loader,\n",
    "        'train_len': len(train_loader),\n",
    "        'val_len': len(val_loader),\n",
    "        'test_len': len(test_loader),\n",
    "    }\n",
    "\n",
    "print(f\"Prepared {len(client_splits)} clients from clusters: {list(client_splits.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66b1d5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: validation loss 0.03439931385219097, mse 0.08461722855766614\n",
      "Epoch 2: validation loss 0.03231489968796571, mse 0.08159168809652328\n",
      "Epoch 3: validation loss 0.031362141172091164, mse 0.08036963765819867\n",
      "Epoch 4: validation loss 0.03066612035036087, mse 0.07993355020880699\n",
      "Epoch 5: validation loss 0.03094364112863938, mse 0.08020769680539767\n",
      "Final test set performance:\n",
      "\tloss 0.02644224651157856\n",
      "\tmse 0.06955851862827937\n"
     ]
    }
   ],
   "source": [
    "# Plain PyTorch training/evaluation loops\n",
    "import torch.nn as nn\n",
    "\n",
    "def train(\n",
    "        model: nn.Module, \n",
    "        loader: DataLoader, \n",
    "        epochs: int, \n",
    "        device) -> float:\n",
    "    \n",
    "    criterion = DecayMSE(lambda_decay=0.95)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion.to(device)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total, epoch_loss = 0.0, 0.0\n",
    "        for x, y, _ in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            assert y_hat.shape == y.shape, f\"y_hat {y_hat.shape} vs y {y.shape}\"\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metric\n",
    "            epoch_loss += loss.item()\n",
    "            total += x.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        # print(f\"Epoch: {epoch+1}, Loss: {epoch_loss:.6f}, Samples: {total}\")\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device) -> Tuple[float, float]:\n",
    "    total_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    n_batches = 0\n",
    "    criterion = DecayMSE(lambda_decay=0.95)\n",
    "    criterion.to(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    mse = nn.MSELoss()\n",
    "    with torch.inference_mode():\n",
    "        for x, y, _ in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            total_loss += loss.item()\n",
    "            total_mse += mse(y_hat, y).item()\n",
    "            n_batches += 1\n",
    "    avg_loss = total_loss / max(1, n_batches)\n",
    "    avg_mse = total_mse / max(1, n_batches)\n",
    "    return avg_loss, avg_mse\n",
    "\n",
    "trainloader, valloader, testloader = client_splits[0]['train_loader'], client_splits[0]['val_loader'], client_splits[0]['test_loader']\n",
    "net = LSTMModel(\n",
    "    input_size=INPUT_SIZE,\n",
    "    output_horizon=DECODER_LENGTH,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    ")\n",
    "\n",
    "for epoch in range(5):\n",
    "    train(net, trainloader, 3, DEVICE)\n",
    "    loss, mse = evaluate(net, valloader, DEVICE)\n",
    "    print(f\"Epoch {epoch+1}: validation loss {loss}, mse {mse}\")\n",
    "\n",
    "loss, mse = evaluate(net, testloader, DEVICE)\n",
    "print(f\"Final test set performance:\\n\\tloss {loss}\\n\\tmse {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56a7ae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flower NumPyClient wrapping local training\n",
    "from collections import OrderedDict\n",
    "\n",
    "class FLClient(NumPyClient):\n",
    "    def __init__(self, net: nn.Module, loaders: Dict[str, DataLoader], epochs: int, lr: float = 1e-4):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.epochs = epochs\n",
    "        self.model = net\n",
    "        self.loaders = loaders\n",
    "        self.model\n",
    "\n",
    "    def get_parameters(self, config: Dict[str, str] = None) -> List[np.ndarray]:\n",
    "        params = [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "        return params\n",
    "\n",
    "    def set_parameters(self, parameters: List[np.ndarray]):\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        train(self.model, self.loaders['train_loader'], self.epochs, self.device)\n",
    "        return self.get_parameters({}), self.loaders['train_len'], {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        loss, mse = evaluate(self.model, self.loaders['test_loader'], self.device)\n",
    "        num_examples = self.loaders['test_len']\n",
    "        return float(loss), num_examples, {\"mse\": float(mse)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42468a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model constructor compatible with FL clients\n",
    "INPUT_SIZE = len(FEATURE_COLS)\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 8\n",
    "NUM_ROUNDS = 7\n",
    "cluster_ids = list(sorted(client_splits.keys()))\n",
    "NUM_CLIENTS = len(cluster_ids)\n",
    "\n",
    "\n",
    "def make_model():\n",
    "    m = LSTMModel(\n",
    "        input_size=INPUT_SIZE,\n",
    "        output_horizon=DECODER_LENGTH,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS\n",
    "    )\n",
    "    return m\n",
    "\n",
    "# Map cluster index to client id list for deterministic ordering\n",
    "\n",
    "def client_fn(context: Context) -> Client:\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    net = make_model()\n",
    "    net.to(DEVICE)\n",
    "    cluster = cluster_ids[partition_id]\n",
    "    loaders = client_splits[cluster]\n",
    "    return FLClient(net, loaders, epochs=3).to_client()\n",
    "\n",
    "client_app = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a4ec6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "strategy = FedAvg(\n",
    "    fraction_fit=1.0,  # use all clients each round (few clusters)\n",
    "    fraction_evaluate=1.0,\n",
    "    min_fit_clients=2,\n",
    "    min_evaluate_clients=5,\n",
    "    min_available_clients=len(cluster_ids),\n",
    ")\n",
    "# Server config\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    # Strategy and simulation config\n",
    "\n",
    "    config = ServerConfig(num_rounds=NUM_ROUNDS)\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e9cd6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=5, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[93mWARNING \u001b[0m:   No evaluate_metrics_aggregation_fn provided\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 5 clients (out of 5)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 5 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 5 round(s) in 27.47s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.07451522297092847\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.03335276964519705\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 3: 0.026256822315709933\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 4: 0.025963755989713327\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 5: 0.025927954353392124\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    }
   ],
   "source": [
    "\n",
    "backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}\n",
    "\n",
    "# When running on GPU, assign an entire GPU for each client\n",
    "if DEVICE == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0}}\n",
    "    print(\"Using GPU for clients\")\n",
    "\n",
    "\n",
    "# Start simulation\n",
    "run_simulation( \n",
    "    server_app=server,\n",
    "    client_app=client_app,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8f82b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline single-epoch model — loss: 0.026127, mse: 0.065338\n"
     ]
    }
   ],
   "source": [
    "# Aggregate global parameters from the last round and evaluate on concatenated test set\n",
    "# Note: Flower simulation returns metrics in `history`; for simplicity, we'll re-run a global model\n",
    "# initialized and averaged by FedAvg inside the server by querying one finished client.\n",
    "\n",
    "# Build a fresh global model and set to parameters from client 0 after final round\n",
    "# (In real setups, pull from strategy weights; Flower 1.x simplifies via client evaluation path.)\n",
    "\n",
    "# For demo, request parameters from one client by re-instantiating and evaluating with averaged server params is non-trivial.\n",
    "# Instead, we can simulate global evaluation by averaging client weights explicitly if needed.\n",
    "\n",
    "# Optional: Evaluate a locally trained model (non-FL) to compare baselines\n",
    "baseline_model = make_model().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-4)\n",
    "criterion = DecayMSE(lambda_decay=0.95)\n",
    "\n",
    "# Quick single-epoch baseline on concatenated train loaders (not required, but useful)\n",
    "# Concatenate all client train loaders into one iterable\n",
    "concat_batches = []\n",
    "for cid in cluster_ids:\n",
    "    for batch in client_splits[cid]['train_loader']:\n",
    "        concat_batches.append(batch)\n",
    "\n",
    "baseline_model.train()\n",
    "for x, y, _ in concat_batches:\n",
    "    x = x.to(DEVICE)\n",
    "    y = y.to(DEVICE)\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = baseline_model(x)\n",
    "    loss = criterion(y_hat, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate on concatenated test loaders\n",
    "test_loader_all = []\n",
    "for cid in cluster_ids:\n",
    "    test_loader_all.append(client_splits[cid]['test_loader'])\n",
    "\n",
    "# Simple evaluation loop over all test loaders\n",
    "@torch.no_grad()\n",
    "def eval_all_loaders(model, loaders: List[DataLoader]):\n",
    "    model.eval()\n",
    "    mse = nn.MSELoss()\n",
    "    total_loss, total_mse, n_batches = 0.0, 0.0, 0\n",
    "    for loader in loaders:\n",
    "        for x, y, _ in loader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            y_hat = model(x)\n",
    "            total_loss += criterion(y_hat, y).item()\n",
    "            total_mse += mse(y_hat, y).item()\n",
    "            n_batches += 1\n",
    "    return total_loss / max(1, n_batches), total_mse / max(1, n_batches)\n",
    "\n",
    "b_loss, b_mse = eval_all_loaders(baseline_model, test_loader_all)\n",
    "print(f\"Baseline single-epoch model — loss: {b_loss:.6f}, mse: {b_mse:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ra-lab (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
